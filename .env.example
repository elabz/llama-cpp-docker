# Common settings
GGML_CUDA_NO_PINNED=1
LLAMA_N_GPU_LAYERS=99
LLAMA_THREADS=3
LLAMA_LOG_FILE=llama

# Completion model settings
COMPLETION_MODEL=/models/Phi-4-mini-instruct.Q8_0.gguf
COMPLETION_CTX_SIZE=8192
COMPLETION_BATCH_SIZE=1024
COMPLETION_UBATCH_SIZE=512
COMPLETION_MAX_PROMPT_TOKENS=4096   # 1/2 for prompt - this should come from client side
COMPLETION_MAX_TOKENS=2048          # 1/4 for generation, 1/4 buffer

# Embedding model settings
EMBEDDING_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
EMBEDDING_CTX_SIZE=4096
EMBEDDING_BATCH_SIZE=2048
EMBEDDING_UBATCH_SIZE=1024

# Qdrant settings
QDRANT_GPU_MEMORY_FRACTION=0.75
QDRANT_GPU_INDEXING=1