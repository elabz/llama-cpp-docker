version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ../logs/nginx:/var/log/nginx
    networks:
      - llms

  llama1:
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/Gemma2-Gutenberg-Doppel-9B-Q6_K.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOGDIR=/logs/
      - LLAMA_LOG_NEW=
      - LLAMA_LOG_ENABLE=
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama2:
    image: llama-cpp-docker
    container_name: llama2
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/Gemma2-Gutenberg-Doppel-9B-Q6_K.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOGDIR=/logs/
      - LLAMA_LOG_NEW=
      - LLAMA_LOG_ENABLE=
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW=
      - LLAMA_LOG_ENABLE=
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDINGS=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama4:
    image: llama-cpp-docker
    container_name: llama4
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW=
      - LLAMA_LOG_ENABLE=
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDINGS=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

networks:
  llms:
    name: llms