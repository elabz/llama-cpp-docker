version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ./logs/nginx:/var/log/nginx
    networks:
      - llms
  redis:
    build:
      context: ./redis
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - ./redis:/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - llms

  llama1:
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/phi-4-Q3_K_M.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=512
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama2:
    image: llama-cpp-docker
    container_name: llama2
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/phi-4-Q3_K_M.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=512
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDINGS=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama4:
    image: llama-cpp-docker
    container_name: llama4
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=4
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDINGS=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

networks:
  llms:
    name: llms
