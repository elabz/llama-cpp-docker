version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ./logs/nginx:/var/log/nginx
    networks:
      - llms

  qdrant:
    build: ./qdrant
    container_name: qdrant
    runtime: nvidia   # Remove for AMD GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage
      - ./qdrant/config:/qdrant/config
    environment:
      - QDRANT__STORAGE__OPTIMIZERS__GPU_MEMORY_FRACTION=0.75
      - QDRANT__GPU__INDEXING=1
    command: ./qdrant --config-path /qdrant/config/config.yaml
    mem_limit: 24g
    networks:
      - llms

  llama1:
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/Dolphin3.0-Llama3.2-3B-Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama2:
    image: llama-cpp-docker
    container_name: llama2
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=8192
      - LLAMA_MODEL=/models/Dolphin3.0-Llama3.2-3B-Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDING=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama4:
    image: llama-cpp-docker
    container_name: llama4
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/nomic-embed-text-v1.5.Q8_0.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=2048
      - LLAMA_UBATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOG_FILE=llama
      - LLAMA_EMBEDDING=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

networks:
  llms:
    name: llms
volumes:
  qdrant_data: