version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ./logs/nginx:/var/log/nginx
    networks:
      - llms
    depends_on:
      - llama1
      - llama2
      - llama3
      - llama4
      - llama5
      - llama6
      - llama7
      - llama8

  qdrant:
    build: ./qdrant
    container_name: qdrant
    runtime: nvidia   # Remove for AMD GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage
      - ./qdrant/config:/qdrant/config
    environment:
      - QDRANT__STORAGE__OPTIMIZERS__GPU_MEMORY_FRACTION=${QDRANT_GPU_MEMORY_FRACTION}
      - QDRANT__GPU__INDEXING=${QDRANT_GPU_INDEXING}
    command: ./qdrant --config-path /qdrant/config/config.yaml
    mem_limit: 24g
    networks:
      - llms

  # Completion model containers (llama1-llama4)
  llama1: &completion_model_template
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${COMPLETION_CTX_SIZE}
      - LLAMA_N_PREDICT=${COMPLETION_MAX_TOKENS}
      - LLAMA_MODEL=${COMPLETION_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${COMPLETION_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${COMPLETION_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama1
      - LLAMA_JINJA=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama2:
    <<: *completion_model_template
    container_name: llama2
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${COMPLETION_CTX_SIZE}
      - LLAMA_N_PREDICT=${COMPLETION_MAX_TOKENS}
      - LLAMA_MODEL=${COMPLETION_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${COMPLETION_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${COMPLETION_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama2
      - LLAMA_JINJA=

  llama3:
    <<: *completion_model_template
    container_name: llama3
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${COMPLETION_CTX_SIZE}
      - LLAMA_N_PREDICT=${COMPLETION_MAX_TOKENS}
      - LLAMA_MODEL=${COMPLETION_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${COMPLETION_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${COMPLETION_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama3
      - LLAMA_JINJA=

  llama4:
    <<: *completion_model_template
    container_name: llama4
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${COMPLETION_CTX_SIZE}
      - LLAMA_N_PREDICT=${COMPLETION_MAX_TOKENS}
      - LLAMA_MODEL=${COMPLETION_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${COMPLETION_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${COMPLETION_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama4
      - LLAMA_JINJA=

  # Embedding model containers (llama5-llama8)
  llama5: &embedding_model_template
    image: llama-cpp-docker
    container_name: llama5
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${EMBEDDING_CTX_SIZE}
      - LLAMA_MODEL=${EMBEDDING_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${EMBEDDING_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${EMBEDDING_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama5
      - LLAMA_EMBEDDING=
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama6:
    <<: *embedding_model_template
    container_name: llama6
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${EMBEDDING_CTX_SIZE}
      - LLAMA_MODEL=${EMBEDDING_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${EMBEDDING_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${EMBEDDING_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama6
      - LLAMA_EMBEDDING=

  llama7:
    <<: *embedding_model_template
    container_name: llama7
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${EMBEDDING_CTX_SIZE}
      - LLAMA_MODEL=${EMBEDDING_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${EMBEDDING_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${EMBEDDING_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama7
      - LLAMA_EMBEDDING=

  llama8:
    <<: *embedding_model_template
    container_name: llama8
    environment:
      - GGML_CUDA_NO_PINNED=${GGML_CUDA_NO_PINNED}
      - LLAMA_CTX_SIZE=${EMBEDDING_CTX_SIZE}
      - LLAMA_MODEL=${EMBEDDING_MODEL}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS}
      - LLAMA_BATCH_SIZE=${EMBEDDING_BATCH_SIZE}
      - LLAMA_UBATCH_SIZE=${EMBEDDING_UBATCH_SIZE}
      - LLAMA_THREADS=${LLAMA_THREADS}
      - LLAMA_LOG_FILE=${LLAMA_LOG_FILE}_llama8
      - LLAMA_EMBEDDING=

networks:
  llms:
    name: llms

volumes:
  qdrant_data: