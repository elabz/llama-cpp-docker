version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ../logs/nginx:/var/log/nginx
  llama1:
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama
    volumes:
      - ./models:/models
      - ./logs:/logs
  llama2:
    image: llama-cpp-docker
    container_name: llama2
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs


  llama4:
    image: llama-cpp-docker
    container_name: llama4
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs

  llama5:
    image: llama-cpp-docker
    container_name: llama5
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs

  llama6:
    image: llama-cpp-docker
    container_name: llama6
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs


  llama7:
    image: llama-cpp-docker
    container_name: llama7
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs

  llama8:
    image: llama-cpp-docker
    container_name: llama8
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_BATCH_SIZE=1024
      - LLAMA_THREADS=3
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
