version: "3.9"
services:
  nginx:
    image: nginx:latest
    container_name: proxy
    ports:
      - "8110:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./logs/nginx:/var/log/nginx
  llama1:
    image: llama-cpp-docker
    container_name: llama1
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama
    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8080:8080"
  llama2:
    image: llama-cpp-docker
    container_name: llama2
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8081:8080"

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8082:8080"

  llama4:
    image: llama-cpp-docker
    container_name: llama4
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8083:8080"

  llama5:
    image: llama-cpp-docker
    container_name: llama5
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8084:8080"

  llama6:
    image: llama-cpp-docker
    container_name: llama6
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8085:8080"

  llama7:
    image: llama-cpp-docker
    container_name: llama7
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8086:8080"

  llama8:
    image: llama-cpp-docker
    container_name: llama8
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
      - LLAMA_LOGDIR=/logs
      - LLAMA_LOG_NEW
      - LLAMA_LOG_ENABLE
      - LLAMA_LOG_FILE=llama

    volumes:
      - ./models:/models
      - ./logs:/logs
    ports:
      - "8087:8080"
