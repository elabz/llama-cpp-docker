version: "3.9"
services:
  nginx:
    build:
      context: ./nginx
    image: nginx:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8110:80"
    volumes:
      - ./logs/nginx:/var/log/nginx
    networks:
      - llms

  qdrant:
    build: ./qdrant
    container_name: qdrant
    runtime: nvidia   # Remove for AMD GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage
      - ./qdrant/config:/qdrant/config
    environment:
      - QDRANT__STORAGE__OPTIMIZERS__GPU_MEMORY_FRACTION=0.75
      - QDRANT__GPU__INDEXING=1
    command: ./qdrant --config-path /qdrant/config/config.yaml
    mem_limit: 24g
    networks:
      - llms

  llama1:
    image: llama-cpp-docker
    container_name: llama1
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
    command:
      - "-m"
      - "/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf"
      - "-c"
      - "16384"
      - "--n-gpu-layers"
      - "99"
      - "t"
      - "4"
      - "--batch-size"
      - "2048"
      - "--ubatch-size"
      - "1024"
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama2:
    image: llama-cpp-docker
    container_name: llama2
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
    command:
      - "-m"
      - "/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf"
      - "-c"
      - "16384"
      - "--n-gpu-layers"
      - "99"
      - "t"
      - "4"
      - "--batch-size"
      - "2048"
      - "--ubatch-size"
      - "1024"
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama3:
    image: llama-cpp-docker
    container_name: llama3
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
    command:
      - "-m"
      - "/models/nomic-embed-text-v1.5.Q8_0.gguf"
      - "-c"
      - "4096"
      - "--n-gpu-layers"
      - "99"
      - "-t"
      - "4"
      - "--embedding"
      - "--batch-size"
      - "2048"
      - "--ubatch-size"
      - "1024"
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

  llama4:
    image: llama-cpp-docker
    container_name: llama4
    restart: unless-stopped
    environment:
      - GGML_CUDA_NO_PINNED=1
    command:
      - "-m"
      - "/models/nomic-embed-text-v1.5.Q8_0.gguf"
      - "-c"
      - "4096"
      - "--n-gpu-layers"
      - "99"
      - "-t"
      - "4"
      - "--embedding"
      - "--batch-size"
      - "2048"
      - "--ubatch-size"
      - "1024"
    volumes:
      - ./models:/models
      - ./logs:/logs
    networks:
      - llms

networks:
  llms:
    name: llms
volumes:
  qdrant_data: