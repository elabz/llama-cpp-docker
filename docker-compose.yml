version: "3.9"
services:
  llama1:
    image: llama-cpp-docker
    container_name: llama1
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
    volumes:
      - ./models:/models
    ports:
      - "8080:8080"
  llama2:
    image: llama-cpp-docker
    container_name: llama2
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
    volumes:
      - ./models:/models
    ports:
      - "8081:8080"
  llama3:
    image: llama-cpp-docker
    container_name: llama3
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=4096
      - LLAMA_MODEL=/models/L3-8B-Stheno-v3.2-Q5_K_M-imat.gguf
      - LLAMA_N_GPU_LAYERS=99
    volumes:
      - ./models:/models
    ports:
      - "8082:8080"
